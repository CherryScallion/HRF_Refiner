# æ–‡ä»¶å: debug_tccc_visualize.py
import torch
import numpy as np
import h5py
import matplotlib.pyplot as plt
from pathlib import Path
from models import create_unet, EEGEncoder, fMRIDecoder, EEG2fMRINet

# ================= é…ç½® =================
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
DATA_ROOT = Path(r"D:\æ¨ç‰¹æ•°æ®çˆ¬å–\E2fNet-main\Data\H5_Unified")
MODEL_PATH = Path(r"D:\æ¨ç‰¹æ•°æ®çˆ¬å–\E2fNet-main\pre\latest_Unified.pth") 
# ä½¿ç”¨ Test Chunk 0
TEST_FILE = DATA_ROOT / "Unified_Test_Chunk_0.h5"

def visualize_time_course():
    print(f"Loading Test File: {TEST_FILE}")
    with h5py.File(TEST_FILE, 'r') as f:
        eeg_raw = f['eeg'][:]
        fmri_gt = f['fmri'][:]
    
    # è·å–å½’ä¸€åŒ–å‚æ•° (ç²—ç•¥)
    min_eeg, max_eeg = eeg_raw.min(), eeg_raw.max()
    eeg_norm = (eeg_raw - min_eeg) / (max_eeg - min_eeg + 1e-10)
    
    # åŠ è½½æ¨¡å‹
    print("Loading Model...")
    eeg_encoder = EEGEncoder(in_channels=20, img_size=64)
    unet_module = create_unet(in_channels=256, out_channels=256)
    fmri_decoder = fMRIDecoder(in_channels=256, out_channels=32)
    model = EEG2fMRINet(eeg_encoder, unet_module, fmri_decoder).to(DEVICE)
    model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))
    model.eval()
    
    # æ¨ç† (å–å‰ 200 ä¸ªæ—¶é—´ç‚¹ï¼Œæ–¹ä¾¿ç”»å›¾)
    time_steps = 200
    if len(eeg_norm) < time_steps: time_steps = len(eeg_norm)
    
    input_tensor = torch.from_numpy(eeg_norm[:time_steps].astype(np.float32)).to(DEVICE)
    
    print("Running Inference...")
    with torch.no_grad():
        # åˆ†æ‰¹è·‘é˜²æ­¢çˆ†æ˜¾å­˜
        pred_list = []
        for i in range(0, time_steps, 32):
            batch = input_tensor[i:i+32]
            pred = model(batch)
            pred_list.append(pred.cpu().numpy())
        pred_fmri = np.concatenate(pred_list, axis=0) # [200, 32, 64, 64]
        gt_fmri = fmri_gt[:time_steps]
        
    # --- è¯Šæ–­æ ¸å¿ƒï¼šæ‰¾å‡ ä¸ªæ´»è·ƒç‚¹ç”»å‡ºæ¥ ---
    
    # 1. æ‰¾åˆ° GT å˜å¼‚æœ€å¤§çš„ä½“ç´  (æœ€æ´»è·ƒçš„è„‘åŒº)
    # è®¡ç®—æ¯ä¸ªä½“ç´ çš„æ—¶é—´æ–¹å·®
    gt_var = np.var(gt_fmri, axis=0) # [32, 64, 64]
    
    # å±•å¹³æ‰¾ç´¢å¼•
    flat_indices = np.argsort(gt_var.flatten())[::-1] # é™åº
    top_indices = flat_indices[:5] # å–æœ€æ´»è·ƒçš„ 5 ä¸ªç‚¹
    
    plt.figure(figsize=(15, 10))
    
    for i, idx in enumerate(top_indices):
        # åè§£åæ ‡
        c, h, w = np.unravel_index(idx, (32, 64, 64))
        
        # æå–æ›²çº¿
        gt_curve = gt_fmri[:, c, h, w]
        pred_curve = pred_fmri[:, c, h, w]
        
        # ç»Ÿè®¡æ•°æ®
        gt_std = np.std(gt_curve)
        pred_std = np.std(pred_curve)
        correlation = np.corrcoef(gt_curve, pred_curve)[0, 1]
        
        plt.subplot(5, 1, i+1)
        plt.plot(gt_curve, label='Ground Truth', color='black', alpha=0.7)
        plt.plot(pred_curve, label=f'Prediction (Corr={correlation:.2f})', color='red', linewidth=2)
        plt.title(f"Voxel [{c},{h},{w}] - GT_Std: {gt_std:.4f} | Pred_Std: {pred_std:.4f}")
        plt.legend(loc='upper right')
        
    plt.tight_layout()
    plt.savefig('debug_time_course.png')
    print("å›¾è¡¨å·²ä¿å­˜ä¸º debug_time_course.pngï¼Œè¯·æŸ¥çœ‹ï¼")
    
    # æ‰“å°æ•°å€¼è¯Šæ–­
    print("\n=== æ•°å€¼è¯Šæ–­ ===")
    print(f"GT å¹³å‡æ ‡å‡†å·® (å…¨è„‘): {np.mean(np.std(gt_fmri, axis=0)):.6f}")
    print(f"Pred å¹³å‡æ ‡å‡†å·® (å…¨è„‘): {np.mean(np.std(pred_fmri, axis=0)):.6f}")
    ratio = np.mean(np.std(pred_fmri, axis=0)) / np.mean(np.std(gt_fmri, axis=0))
    print(f"æ–¹å·®æ¯”ç‡ (Pred/GT): {ratio:.4f}")
    
    if ratio < 0.1:
        print("ğŸš¨ ç»“è®º: æ¨¡å‹åå¡Œ (Model Collapse)ã€‚é¢„æµ‹å€¼å‡ ä¹ä¸åŠ¨ï¼Œåªè¾“å‡ºäº†å¹³å‡å›¾åƒã€‚")
    elif correlation < 0.1:
        print("ğŸš¨ ç»“è®º: éšæœºæ³¢åŠ¨ã€‚æ¨¡å‹æœ‰è¾“å‡ºæ³¢åŠ¨ï¼Œä½†å’ŒçœŸå®å€¼å®Œå…¨æ²¡å…³ç³»ã€‚")
    else:
        print("âœ… ç»“è®º: çœ‹èµ·æ¥è¿˜è¡Œï¼Ÿé‚£å¯èƒ½æ˜¯ä¹‹å‰ TCCC è„šæœ¬è®¡ç®—æœ‰è¯¯ã€‚")

if __name__ == "__main__":
    visualize_time_course()